{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ahmed Mustafa\n",
      "[nltk_data]     Malik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Tweet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self):\n",
    "        self.uid = None\n",
    "        self.content = ''\n",
    "        self.sentiment = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definining Methods for Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    # print(tweet.content,\"\\n\")\n",
    "    tweet.content = re.sub(r'\\_', '', tweet.content) # remove underscores\n",
    "    tweet.content = re.sub(r'â€¦', '', tweet.content) # remove elipses/dots\n",
    "    tweet.content = re.sub(r'\\.', '', tweet.content) # remove elipses/dots\n",
    "    tweet.content = re.sub(\"@ [a-zA-Z0-9]+\", '', tweet.content) # remove @mentions\n",
    "    tweet.content = re.sub(r'^RT[\\s]+', '', tweet.content) # remove RT\n",
    "    tweet.content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet.content) # remove hyperlinks\n",
    "    tweet.content = re.sub(r'#', '', tweet.content) # remove hastags\n",
    "    tweet.content = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True).tokenize(tweet.content)\n",
    "    tweet.content = ' '.join([word for word in tweet.content if word not in string.punctuation]) # remove punctuations\n",
    "    # print(tweet.content)\n",
    "    # print(\"============================================================================\")\n",
    "    return tweet\n",
    "\n",
    "def load_stop_words():\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    stopwords_hinglish = []\n",
    "    with open('data/hinglish_stopwords.txt','r') as fp:\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            stopwords_hinglish.append(line.strip())\n",
    "    return stopwords_english, stopwords_hinglish\n",
    "\n",
    "def readFile(filename, test_data=False):\n",
    "    stemmer_english = PorterStemmer()\n",
    "    stopwords_english, stopwords_hinglish = load_stop_words()\n",
    "    all_tweets = []\n",
    "    with open(filename, 'r', encoding=\"utf8\") as fp:\n",
    "        tweet = Tweet()\n",
    "        last_one = False\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            if not line:\n",
    "                last_one = True\n",
    "            if len(line.split()) > 1 or last_one==True:\n",
    "                if last_one==True or line.split()[0] == 'meta':\n",
    "                    if len(tweet.content) > 0 or last_one==True:\n",
    "                        all_tweets.append(cleanTweet(tweet))\n",
    "                        if last_one==True:\n",
    "                            break\n",
    "                        tweet = Tweet()\n",
    "                    tweet.uid = line.split()[1]\n",
    "                    tweet.sentiment = line.split()[2] if test_data==False else None\n",
    "                else:\n",
    "                    if line.split()[1] == \"Eng\":\n",
    "                        if line.split()[0] not in stopwords_english:\n",
    "                            tweet.content += stemmer_english.stem(line.split()[0]) + \" \"\n",
    "                    elif line.split()[1] == \"Hin\":\n",
    "                        if line.split()[0] not in stopwords_hinglish:\n",
    "                            tweet.content += line.split()[0] + \" \"\n",
    "                    else:\n",
    "                        tweet.content += line.split()[0] + \" \"\n",
    "        return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Methods for creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_based_embeddings(all_tweets):\n",
    "    freqs = dict()\n",
    "    for i in all_tweets:\n",
    "        for word in i.content.split():\n",
    "            pair = (word, i.sentiment)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "    all_positive_tweets = [i.content for i in all_tweets if i.sentiment=='positive']\n",
    "    all_negative_tweets = [i.content for i in all_tweets if i.sentiment=='negative']\n",
    "    all_neutral_tweets = [i.content for i in all_tweets if i.sentiment=='neutral']\n",
    "    tweets = all_neutral_tweets + all_positive_tweets + all_negative_tweets\n",
    "    X = np.empty((0,4))\n",
    "    for i in tweets:\n",
    "        neutral_words_freq = 0\n",
    "        positive_words_freq = 0\n",
    "        negative_words_freq = 0\n",
    "        for word in i.split():\n",
    "            neutral_words_freq += freqs.get((word,'neutral'),0)\n",
    "            positive_words_freq += freqs.get((word,'positive'),0)\n",
    "            negative_words_freq += freqs.get((word,'negative'),0)\n",
    "        X = np.append(X, np.array([[1, neutral_words_freq, positive_words_freq, negative_words_freq]]), axis=0)\n",
    "    Y = np.append(np.zeros(len(all_neutral_tweets)), np.ones(len(all_positive_tweets)))\n",
    "    Y = np.append(Y, np.full(len(all_negative_tweets),2))\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Method for Showing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,title='Confusion Matrix',cmap=plt.cm.Greens):\n",
    "    import itertools\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    classes = ['1','2','3']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def show_results(y_test, y_pred):\n",
    "    print(\"F1 Score: \", f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    print()\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "    plot_confusion_matrix(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission Control - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = readFile(r'data\\train\\train_14k_split_conll.txt')\n",
    "all_tweets.extend(readFile(r'data\\train\\dev_3k_split_conll.txt'))\n",
    "all_tweets.extend(readFile(r'data\\train\\train_conll.txt'))\n",
    "all_tweets.extend(readFile(r'data\\train\\trial_conll.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating File for Fast-Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fast-text-data/data.train.txt', 'w', encoding=\"utf-8\") as fp:\n",
    "    for i in all_tweets:\n",
    "        fp.write(\"__label__\"+i.sentiment+\" \"+i.content+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised('data/fast-text-data/data.train.txt',epoch=50,loss='softmax',wordNgrams=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission Control - Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels_dict = dict()\n",
    "with open(r'data\\test\\test_labels_hinglish.txt','r') as fp:\n",
    "    line = fp.readline()\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        actual_labels_dict[line.strip().split(',')[0]] = line.strip().split(',')[1]\n",
    "all_test_tweets = readFile(r'data\\test\\Hindi_test_unalbelled_conll_updated.txt',test_data=True)\n",
    "for i in all_test_tweets:\n",
    "    i.sentiment = actual_labels_dict[i.uid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in all_test_tweets:\n",
    "    predictions.append(model.predict(i.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_num = []\n",
    "for i in all_test_tweets:\n",
    "    if i.sentiment == \"neutral\":\n",
    "        actual_num.append(0)\n",
    "    elif i.sentiment == \"positive\":\n",
    "        actual_num.append(1)\n",
    "    elif i.sentiment == \"negative\":\n",
    "        actual_num.append(2)\n",
    "predictions_num = []\n",
    "for i in predictions:\n",
    "    if i[0][0] == \"__label__neutral\":\n",
    "        predictions_num.append(0)\n",
    "    elif i[0][0] == \"__label__positive\":\n",
    "        predictions_num.append(1)\n",
    "    elif i[0][0] == \"__label__negative\":\n",
    "        predictions_num.append(2)\n",
    "\n",
    "show_results(actual_num, predictions_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answer.txt',\"w\") as fp:\n",
    "    fp.write(\"Uid,Sentiment\\n\")\n",
    "    for i in range(len(predictions_num)):\n",
    "        if predictions_num[i]==0:\n",
    "            fp.write(all_test_tweets[i].uid+\",neutral\\n\")\n",
    "        elif predictions_num[i]==1:\n",
    "            fp.write(all_test_tweets[i].uid+\",positive\\n\")\n",
    "        elif predictions_num[i]==2:\n",
    "            fp.write(all_test_tweets[i].uid+\",negative\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

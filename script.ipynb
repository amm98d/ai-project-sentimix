{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ahmed Mustafa\n[nltk_data]     Malik\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "from nltk import download\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self):\n",
    "        self.ID = None\n",
    "        self.content = ''\n",
    "        self.sentiment = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet):\n",
    "    # print(tweet.content,\"\\n\")\n",
    "    tweet.content = re.sub(\"@ [a-zA-Z0-9]+\", '', tweet.content) # remove @mentions\n",
    "    tweet.content = re.sub(r'^RT[\\s]+', '', tweet.content) # remove RT\n",
    "    tweet.content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet.content) # remove hyperlinks\n",
    "    tweet.content = re.sub(r'#', '', tweet.content) # remove hastags\n",
    "    tweet.content = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True).tokenize(tweet.content)\n",
    "    tweet.content = ' '.join([word for word in tweet.content if word not in string.punctuation]) # remove punctuations\n",
    "    # print(tweet.content)\n",
    "    # print(\"============================================================================\")\n",
    "    return tweet"
   ]
  },
  {
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "stopwords_hinglish = []\n",
    "with open('data/hinglish_stopwords.txt','r') as fp:\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        stopwords_hinglish.append(line.strip())\n",
    "stemmer_english = PorterStemmer()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tweets = []\n",
    "with open('data/trial_conll.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    tweet = Tweet()\n",
    "    skip = False\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if len(line.split()) > 1:\n",
    "            if line.split()[0] == 'meta':\n",
    "                skip = False\n",
    "                if len(tweet.content) > 0:\n",
    "                    all_tweets.append(cleanTweet(tweet))\n",
    "                    tweet = Tweet()\n",
    "                tweet.ID = line.split()[1]\n",
    "                tweet.sentiment = line.split()[2]\n",
    "            elif line.split()[0] == 'http' or line.split()[0] == 'https':\n",
    "                skip = True\n",
    "            elif skip == False:\n",
    "                if line.split()[1] == \"Eng\":\n",
    "                    if line.split()[0] not in stopwords_english:\n",
    "                        tweet.content += stemmer_english.stem(line.split()[0]) + \" \"\n",
    "                elif line.split()[1] == \"Hin\":\n",
    "                    if line.split()[0] not in stopwords_hinglish:\n",
    "                        tweet.content += line.split()[0] + \" \"\n",
    "                else:\n",
    "                    tweet.content += line.split()[0] + \" \"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'phd you â€™ awesom much love hope apsa'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "all_tweets[50].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "415b62045bf1fc50c392a94e494268ff778fc0a50c0e844b2241a04ffd062c64"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}